{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "logging.getLogger(\"cmdstanpy\").disabled = True\n",
    "import shap\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "import requests\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This challenge consists in identifying trends and insights in a sales dataset and forecasting weekly sales based on a time series.\n",
    "\n",
    "Those were the requirements:\n",
    "1. Propose recommended actions based on the insights obtained, prioritizing those with the greatest business impact.\n",
    "2. Model the effects of discounts during holiday weeks.\n",
    "3. Create an API that allows the store system to query, through an endpoint, the sales forecast for the next four weeks.\n",
    "4. Predict the sales of each department in each store for the next year.\n",
    "\n",
    "The solution was implemented with two different models, which will be explained in the following topics, one for the next year prediction and one for the four weeks API:\n",
    "\n",
    "\n",
    "**Regressor (XGBoost)** - four weeks prediction\n",
    "\n",
    "**Prophet** - next year prediction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Propose recommended actions based on the insights obtained, prioritizing those with the greatest business impact.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Data exploration\n",
    "The dataset consists of 3 csv files containing stores/departments weekly sales from 05/02/2010 to 01/11/2012.\n",
    "\n",
    "The first step was joining this data to gather all the relevant information and saving it into a new dataset (combined_data.csv)\n",
    "\n",
    "The code for joining the data can be found in the notebook data_processing.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataframe containing the joined data\n",
    "df = pd.read_csv('../data/combined_data.csv')\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With the combined data it was possible to start visualizing the overall weekly sales trends over the years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_sales = df.groupby('Date')['Weekly_Sales'].sum().reset_index()\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(data=weekly_sales, x='Date', y='Weekly_Sales', marker='o', color='b')\n",
    "plt.title('Weekly sales over time', fontsize=16)\n",
    "plt.xlabel('weeks', fontsize=12)\n",
    "plt.ylabel('Total sales', fontsize=12)\n",
    "plt.xticks(weekly_sales.index[::4],  \n",
    "           weekly_sales['Date'].iloc[::4],\n",
    "           rotation=45,\n",
    "           ha='right')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_sales_by_week = df.groupby('week')['Weekly_Sales'].mean().reset_index()\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(data=weekly_sales_by_week, x='week', y='Weekly_Sales', marker='o', color='b')\n",
    "plt.title('Mean weekly sales trend by week of year', fontsize=16)\n",
    "plt.xlabel('week', fontsize=12)\n",
    "plt.ylabel('Mean sales', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Feature analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "week_sales = df.groupby(\"Date\")[\"Weekly_Sales\"].sum()\n",
    "top_5_total_sales = week_sales.sort_values(ascending=False).head(10)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=top_5_total_sales.index, y=top_5_total_sales.values)\n",
    "plt.xlabel(\"weeks\")\n",
    "plt.ylabel(\"Total weekly sales\")\n",
    "plt.title(\"Total weekly sales for top 10 weeks\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The data shows that the highest sales peaks happens on the thanksgiving and christmas weeks, with a drop in the begging of the year, starting to recover on february with the super bowl week "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The next step was to analyze the influence of external factors (temperature, fuel price, CPI, unemployment) into the weekly sales "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr = df[['Weekly_Sales', 'Temperature', 'Fuel_Price', 'CPI', 'Unemployment']]\n",
    "correlation_matrix = df_corr.corr()\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', vmin=-1, vmax=1, cbar=True)\n",
    "plt.title('Correlation between environmental factors and sales', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The data shows that there is almost no correlation between enviroment factores and sales, with unemployment and CPI the only two with a correlation above zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holiday_sales = df.groupby('IsHoliday')['Weekly_Sales'].mean()\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(x=holiday_sales.index, y=holiday_sales.values, palette='viridis', hue=holiday_sales.index, legend=False)\n",
    "plt.title('Average weekly sales: holiday vs. non-holiday', fontsize=16)\n",
    "plt.xlabel('Is holiday', fontsize=12)\n",
    "plt.ylabel('Average weekly sales', fontsize=12)\n",
    "plt.xticks([0, 1], labels=['Non-holiday', 'holiday'], fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Holidays, on the other hand, result in a 6.5% increase on the avarage weekly sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markdowns (discounts) also show a relevant correlation with weekly sales as seen bellow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_data = df.dropna()\n",
    "df_corr = markdown_data[['Weekly_Sales', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']]\n",
    "correlation_matrix = df_corr.corr()\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', vmin=-1, vmax=1, cbar=True)\n",
    "plt.title('Correlation between markdowns and sales', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It`s also possible to analyze the relations beetween holidays and Markdowns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_mean(values):\n",
    "    return values.mean() if len(values.dropna()) > 0 else 0\n",
    "markdowns_effect = df.groupby(['IsHoliday']).agg({\n",
    "    'MarkDown1': calc_mean,\n",
    "    'MarkDown2': calc_mean,\n",
    "    'MarkDown3': calc_mean,\n",
    "    'MarkDown4': calc_mean,\n",
    "    'MarkDown5': calc_mean,\n",
    "    'Weekly_Sales': 'mean'\n",
    "})\n",
    "markdowns_effect = markdowns_effect.reset_index()\n",
    "plt.figure(figsize=(14, 8))\n",
    "markdown_columns = ['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']\n",
    "for i, column in enumerate(markdown_columns, 1):\n",
    "    plt.subplot(2, 3, i)  \n",
    "    plt.bar(markdowns_effect['IsHoliday'].astype(str), markdowns_effect[column], color=['blue', 'orange'])\n",
    "    plt.title(f'Effect of {column}')\n",
    "    plt.ylabel('Average Value')\n",
    "    plt.xlabel('IsHoliday')\n",
    "plt.suptitle('Markdown avg values: holiday vs non-Holiday', fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markdown3 and Markdown2 are the ones mostly present only during holidays weeks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It is also possible to model a regressor using only MarkDown and Store/Dept features \n",
    "### The Markdown NaN values were replaced by zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'Store', 'Dept']\n",
    "X = df[features].fillna(0) \n",
    "y = df['Weekly_Sales']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f\"RMSE: {rmse}\")\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "importance = pd.Series(model.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "print(\"Feature Importance:\\n\", importance)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model's feature importances show that MarkDown5 and MarkDown3 are the most important MarkDowns to predict a store/dept weekly sales value.\n",
    "\n",
    "### As seen bellow, those features together with store/department information are enough to create a regressor model that can predict weekly sales really well. The MSE and MAE are relativelly low compared to the avarege weekly sales values, the the R-squared value is close to 1, meaning that the model explains nearly all the variability in the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "print(f\"R-squared (R2): {r2:.2f}\")\n",
    "residuals = y_test - y_pred\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(residuals, kde=True, bins=30)\n",
    "plt.title('Residual Distribution')\n",
    "plt.xlabel('Residuals')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.6)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "plt.title('Predicted vs True Values')\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predictions')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With SHAP explainer the positive effects of MarkDown3 and MarkDown5 on the weekly sales are clearer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(model)\n",
    "subset = X_test.sample(n=100, random_state=42) \n",
    "shap_values = explainer.shap_values(subset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, subset)\n",
    "shap.force_plot(explainer.expected_value, shap_values[0], subset.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally we can analyze the weekly sales by store and department"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_by_store_dept = df.groupby(['Store', 'Dept'])['Weekly_Sales'].sum().reset_index()\n",
    "sales_by_store_dept = sales_by_store_dept.sort_values(by='Weekly_Sales', ascending=False)\n",
    "top_stores = sales_by_store_dept.groupby('Store')['Weekly_Sales'].sum().nlargest(10).reset_index()\n",
    "top_departments = sales_by_store_dept.groupby('Dept')['Weekly_Sales'].sum().nlargest(10).reset_index()\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Store', y='Weekly_Sales', data=top_stores, palette='viridis', hue='Store')\n",
    "plt.title('Top 10 Stores by weekly sales')\n",
    "plt.xlabel('Store')\n",
    "plt.ylabel('Total weekly sales')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Dept', y='Weekly_Sales', data=top_departments, palette='viridis', hue='Dept')\n",
    "plt.title('Top 10 Departments by weekly Sales')\n",
    "plt.xlabel('Department')\n",
    "plt.ylabel('Total weekly sales')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Recommended actions\n",
    "### Based on the analyzed data, those are my recommended actions:\n",
    "\n",
    "### - **Optimize markdown strategies**: MarkDowns 3 and 5 are the ones that show the best results for weekly sales creating strategies for the other MarkDows similar to the ones used for 3 and 5 could improve weekly sales\n",
    "### - **Holiday promotions**: weekly sales are considerably higher during holidays, extending Markdowns strategies to pre and post holiday weeks could improve weekly sales during those periods\n",
    "### - **Focus on top performing stores**: Analyze what is being done in stores like 20,14 and 4 and reproduce their strategies to low performing stores\n",
    "### - **Focus on top performing departments**: Analyze what is being done in departments like 92,95 and 38 and reproduce their strategies to low performing departments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model the effects of discounts during holiday weeks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling those effects will be simillar to the modeling of Markdowns done previously, but now for a subset of only holiday weeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holiday_data = df[df['IsHoliday']]\n",
    "\n",
    "features = ['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'Store', 'Dept']\n",
    "X_holiday = holiday_data[features].fillna(0) \n",
    "y_holiday = holiday_data['Weekly_Sales']\n",
    "X_holiday_train, X_holiday_test, y_holiday_train, y_holiday_test = train_test_split(X_holiday, y_holiday, test_size=0.2, random_state=42)\n",
    "model_holiday = RandomForestRegressor(random_state=42)\n",
    "model_holiday.fit(X_holiday_train, y_holiday_train)\n",
    "y_holiday_pred = model_holiday.predict(X_holiday_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_holiday_test, y_holiday_pred))\n",
    "print(f\"RMSE: {rmse}\")\n",
    "mse = mean_squared_error(y_holiday_test, y_holiday_pred)\n",
    "mae = mean_absolute_error(y_holiday_test, y_holiday_pred)\n",
    "r2 = r2_score(y_holiday_test, y_holiday_pred)\n",
    "importance = pd.Series(model_holiday.feature_importances_, index=X_holiday.columns).sort_values(ascending=False)\n",
    "print(\"Feature Importance:\\n\", importance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "print(f\"R-squared (R2): {r2:.2f}\")\n",
    "residuals = y_holiday_test - y_holiday_pred\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(residuals, kde=True, bins=30)\n",
    "plt.title('Residual Distribution')\n",
    "plt.xlabel('Residuals')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_holiday_test, y_holiday_pred, alpha=0.6)\n",
    "plt.plot([y_holiday_test.min(), y_holiday_test.max()], [y_holiday_test.min(), y_holiday_test.max()], 'r--', lw=2)\n",
    "plt.title('Predicted vs True Values')\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predictions')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The results are similar to before, with MarkDown3 and MarkDown5 having the highest impact in predicting weekly sales\n",
    "### The R2 is lower and the residuals are worse, but this can be explained by the lower amount of data for holiday weeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer_holiday = shap.TreeExplainer(model_holiday)\n",
    "subset_holiday = X_holiday_test.sample(n=100, random_state=42) \n",
    "shap_values_holiday = explainer_holiday.shap_values(subset_holiday)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values_holiday, subset)\n",
    "shap.force_plot(explainer_holiday.expected_value, shap_values[0], subset.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For MarkDowns 3 2 and 5, increasing their values most of the time results in incresing the model output (weekly sales). For MarkDowns 1 and 4 this is not always true\n",
    "\n",
    "### This is also shown in the correlation matrix bellow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_data = holiday_data.dropna()\n",
    "df_corr = markdown_data[['Weekly_Sales', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']]\n",
    "correlation_matrix = df_corr.corr()\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', vmin=-1, vmax=1, cbar=True)\n",
    "plt.title('Correlation between markdowns and sales (holidays)', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create an API that allows the store system to query, through an endpoint, the sales forecast for the next four weeks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Time Series Forecasting\n",
    "### Time series Forecasting models are special types of predictive models because there is no data available for future predictions.\n",
    "### Example: In a traditional predictive model (car price prediction) you will train the data with the past data features (color, hp, miles, year, etc), and when predicting a price for a car that was not present in the training data, you will have all those informations to make the new prediction.\n",
    "### In the time series forecasting models, some information will be present on the training data, but some will not be present on the cases you want to predict.\n",
    "### Example: For next year the store and department numbers are known, but the temperature, discounts, fuel price, etc, are not known yet.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Feature Engineering\n",
    "### To overcome this problem it is necessary to create new features called lag features that represent the data from the past (in this case more than 4 weeks in the past) to train the model, so when predicting dates 4 weeks ahead, all the necessary data will be available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function used to create the lag features on the data_processing.ipynb notebook\n",
    "def prepare_forecasting_features_4_weeks(df):\n",
    "    df = df.copy()\n",
    "    shiftable_columns = ['Weekly_Sales', 'Temperature', 'MarkDown1', 'Fuel_Price',\n",
    "                         'MarkDown2', 'MarkDown3', 'MarkDown4', \n",
    "                         'MarkDown5', 'CPI', 'Unemployment']\n",
    "    dropable_columns = ['Temperature', 'MarkDown1', 'Fuel_Price',\n",
    "                        'MarkDown2', 'MarkDown3', 'MarkDown4', \n",
    "                        'MarkDown5', 'CPI', 'Unemployment']\n",
    "    \n",
    "    lags = [4, 8, 16, 32]\n",
    "    for lag in lags:\n",
    "        for col in shiftable_columns:\n",
    "            df[f'lag_{lag}_{col}'] = df.groupby(['Store', 'Dept'])[col].shift(lag)\n",
    "    \n",
    "    feature_df = df.drop(dropable_columns, axis=1)\n",
    "    df_clean = feature_df.dropna(subset=['lag_32_Weekly_Sales'])\n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Model training\n",
    "### With the new features a XGBoost regressor model was trained (full training code can be found on 4_weeks_model_training.ipynb)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Results\n",
    "### Mean Squared Error (MSE): 17788478.90\n",
    "### Mean Absolute Error (MAE): 1834.73\n",
    "### R-squared (R2): 0.97"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residuals:\n",
    "![Residuals](residuals.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### True values vs predictions for X_test (red = true, blue = prediction)\n",
    "![Pred vs true1](predicted_vs_true.png)\n",
    "![Pred vs true2](predicted_vs_true_test.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction examples for specific store/depts \n",
    "![ex1](example1.png)\n",
    "![ex2](example2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature explainer\n",
    "![explained](feature_explained.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model shows excellent predictive accuracy with minimal error (strong fit to the data (high RÂ²) and small errors in its predictions (low MSE and RMSE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 API and prediction endpoint\n",
    "\n",
    "After training and saving the trained model, a class and methods for the model was created on /src/app/model_handler.py\n",
    "\n",
    "The API was created using Flask on /src/app/app.py\n",
    "\n",
    "The endpoint for making a 4 week prediction can be called by doing a POST request on the route /predict (must run ```python app.py``` first on /src/app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input object consists on know data of the 4 weeks/stores/depts you want to predict\n",
    "input_obj= {\n",
    "    'store_data': [\n",
    "        {\n",
    "            'Store': 1,\n",
    "            'Dept': 1,\n",
    "            'IsHoliday': True,\n",
    "            'Type': 'A',\n",
    "            'Size': 1234,\n",
    "            'date': '28/10/2012'\n",
    "        },\n",
    "                {\n",
    "            'Store': 1,\n",
    "            'Dept': 1,\n",
    "            'IsHoliday': False,\n",
    "            'Type': 'A',\n",
    "            'Size': 12345,\n",
    "            'date': '04/11/2011'\n",
    "        },\n",
    "                {\n",
    "            'Store': 1,\n",
    "            'Dept': 1,\n",
    "            'IsHoliday': False,\n",
    "            'Type': 'A',\n",
    "            'Size': 123456,\n",
    "            'date': '11/11/2011'\n",
    "        },\n",
    "                {\n",
    "            'Store': 1,\n",
    "            'Dept': 1,\n",
    "            'IsHoliday': True,\n",
    "            'Type': 'A',\n",
    "            'Size': 123456,\n",
    "            'date': '18/11/2011'\n",
    "        },\n",
    "    ]\n",
    "}\n",
    "\n",
    "url = 'http://127.0.0.1:5000/predict'\n",
    "\n",
    "response = requests.post(url, json=input_obj)\n",
    "\n",
    "print(response.status_code)\n",
    "print(response.json())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Each element of the prediction array is a weekly sales prediction for the following weeks (1 to 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Predict the sales of each department in each store for the following year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "To predict the next year sales it was chosen Facebook's forecasting model [Prophet](https://facebook.github.io/prophet/)\n",
    "\n",
    "The solution using this model can be found on src/app/prophet_handler.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "src_path = Path.cwd().parent / \"src\"\n",
    "sys.path.append(str(src_path))\n",
    "from app.prophet_handler import ProphetModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prophet = ProphetModel()\n",
    "predictions=prophet.predict(store=1,department=5)\n",
    "predictions.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot predictions plot the true values (black dots), and the prediction (blue line)\n",
    "#y= weekly_sales\n",
    "#ds = date\n",
    "prophet.plot_prediction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_2=prophet.predict(store=2,department=5)\n",
    "prophet.plot_prediction()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation and performance metrics\n",
    "\n",
    "Prophet has a cross validation method receives a initial training data period (initial), a cutoff (period), and a prediction horizon (horizon)\n",
    "\n",
    "``` df_cv = cross_validation(self.model, initial='400 days', period='30 days', horizon = '365 days')``` \n",
    "\n",
    "The cross-validation was performed with an initial training period of 400 days, a period of 30 days, and a forecast horizon of 60 days. This means that the model was trained on the first 400 days of data, then evaluated on the next 60 days, and this process was repeated every 30 days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_df = prophet.performance()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Performance\n",
    "\n",
    "The model performance seem to fluctuate, starting with a lower mse/rmse/mae on the short forecasts, increasing the errors on medium term forecast, and decreasing again on longer forecasts.\n",
    "\n",
    "This could be explained by the seaonal effects on the weekly sales (eg. holiday seasons that are closer to the longer forecast for this specific cross validation are easier to predict)\n",
    "\n",
    "Overall the model performance seems reasonable consistent throught the 365 forecasting period, but more in depth analysis would be necessary to understand better its predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting one year ahead for all store/depts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_depts = pd.read_csv('../data/store_dept.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WARNING: this is going to take a long time to run be mindful when uncomenting/running\n",
    "# for _, row in store_depts.iterrows():\n",
    "#     store = row['Store']\n",
    "#     dept = row['Dept']\n",
    "#     predictions=prophet.predict(store=store,department=dept)\n",
    "#     file_path = 'year_store_predictions.csv'\n",
    "\n",
    "#     if os.path.exists(file_path):\n",
    "#         predictions.to_csv(file_path, mode='a', header=False, index=False)\n",
    "#     else:\n",
    "#         predictions.to_csv(file_path, mode='w', header=True, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Future Improvements \n",
    "\n",
    "- For the 4 weeks prediction problem, only one model was trained (XGBoost), although it is a really solid model and the performance was good, other models could be trained/tested for comparison\n",
    "- For the 1 year prediction problem it could have been used the same strategy with the XGBoost model with better performance/interpretability, but Prophet is also a interesting model that I wanted to give a try\n",
    "- The API was really simple only running locally, with only one endpoint and no authentication methods, it served what had been asked but it could be improved in many ways if necessary\n",
    "- Documenting the classes and methods would make it easier for others to understand/improve the code\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
